{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb713f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ChainAbuse BTC Scam‑Report Scraper – Jupyter Edition\n",
    "===================================================\n",
    "\n",
    "Fetches paginated scam reports for the **Bitcoin** chain from\n",
    "<https://www.chainabuse.com> and stores each card (category, description,\n",
    "author, BTC address, associated domain) into *scam_reports.csv*.\n",
    "\n",
    "Key features\n",
    "------------\n",
    "* **Playwright + headless Chromium** – bypasses Cloudflare + dynamic JS.\n",
    "* **Cookie injection** – copy/paste your own `cf_clearance` (and friends)\n",
    "  into the `COOKIE_STR` constant or export `CHAINABUSE_COOKIES`.\n",
    "* **Resumable** – progress is persisted in `last_page.txt`; restart the\n",
    "  notebook to pick up where it left off.\n",
    "* **Tunable politeness** – configurable delays, extra wait every N pages.\n",
    "* **Structured logging** – progress and errors via the `logging` module.\n",
    "* **Jupyter‑friendly** – `# %%` cell delimiters; run top‑to‑bottom.\n",
    "\n",
    "> ⚠️ **Legal notice:** Scraping may violate ChainAbuse’s ToS. Use at your own\n",
    "> risk and respect robots.txt / API if available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Imports & constants\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Load scraped data\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "CSV_PATH = Path(\"scam_reports.csv\")\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"scam_reports.csv not found. Run the scraper first or place the file in the working directory.\"\n",
    "    )\n",
    "\n",
    "# Read using UTF-8 for maximum compatibility\n",
    "# If your file contains a different encoding, adjust the `encoding` argument.\n",
    "df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
    "print(f\"Loaded {len(df):,} rows from {CSV_PATH}\")\n",
    "\n",
    "display(df.head())  # Jupyter-friendly preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2409b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. Bitcoin address validator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "BITCOIN_LEGACY_REGEX = re.compile(r\"^[13][a-km-zA-HJ-NP-Z1-9]{25,34}$\")\n",
    "BITCOIN_BECH32_REGEX = re.compile(r\"^(bc1)[0-9a-z]{25,39}$\")\n",
    "\n",
    "def is_valid_bitcoin_address(address: str) -> bool:\n",
    "    \"\"\"Return **True** if *address* matches a plausible Bitcoin address pattern.\"\"\"\n",
    "    if not isinstance(address, str):\n",
    "        return False\n",
    "    return bool(\n",
    "        BITCOIN_LEGACY_REGEX.match(address) or BITCOIN_BECH32_REGEX.match(address)\n",
    "    )\n",
    "\n",
    "# Vectorised validation\n",
    "ADDRESS_COL = \"Indirizzo Bitcoin\"  # change here if your column has a different name\n",
    "df[\"valid\"] = df[ADDRESS_COL].apply(is_valid_bitcoin_address)\n",
    "\n",
    "print(\n",
    "    f\"Found {df['valid'].sum():,} valid BTC addresses out of {len(df):,} total rows.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea57d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. Clean & de-duplicate\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "filtered_df = (\n",
    "    df[df[\"valid\"]]  # keep valid addresses only\n",
    "    .drop(columns=\"valid\")  # no need to persist helper column\n",
    "    .drop_duplicates(subset=[ADDRESS_COL], keep=\"first\")  # unique addresses\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Rows after filtering: {len(filtered_df):,}\")\n",
    "display(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. Persist cleaned dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_PATH = Path(\"scam_reports_filtered.csv\")\n",
    "filtered_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"✅ Saved cleaned dataset to {OUTPUT_PATH.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
