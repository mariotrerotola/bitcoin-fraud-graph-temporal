{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8839d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ChainAbuse BTC Scam‑Report Scraper – Jupyter Edition\n",
    "===================================================\n",
    "\n",
    "Fetches paginated scam reports for the **Bitcoin** chain from\n",
    "<https://www.chainabuse.com> and stores each card (category, description,\n",
    "author, BTC address, associated domain) into *scam_reports.csv*.\n",
    "\n",
    "Key features\n",
    "------------\n",
    "* **Playwright + headless Chromium** – bypasses Cloudflare + dynamic JS.\n",
    "* **Cookie injection** – copy/paste your own `cf_clearance` (and friends)\n",
    "  into the `COOKIE_STR` constant or export `CHAINABUSE_COOKIES`.\n",
    "* **Resumable** – progress is persisted in `last_page.txt`; restart the\n",
    "  notebook to pick up where it left off.\n",
    "* **Tunable politeness** – configurable delays, extra wait every N pages.\n",
    "* **Structured logging** – progress and errors via the `logging` module.\n",
    "* **Jupyter‑friendly** – `# %%` cell delimiters; run top‑to‑bottom.\n",
    "\n",
    "> ⚠️ **Legal notice:** Scraping may violate ChainAbuse’s ToS.  Use at your own\n",
    "> risk and respect robots.txt / API if available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nest_asyncio.apply()  # allow nested event loops inside notebooks\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration – edit as needed\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE_URL = \"https://www.chainabuse.com/chain/BTC?page={page}\"\n",
    "START_PAGE = 1          # inclusive\n",
    "END_PAGE   = 100        # inclusive; set to 17672 for full scrape\n",
    "\n",
    "CSV_PATH = Path(\"scam_reports.csv\")\n",
    "STATE_PATH = Path(\"last_page.txt\")\n",
    "\n",
    "# Wait settings ---------------------------------------------------------------\n",
    "WAIT_BETWEEN_PAGES = 5       # seconds\n",
    "EXTRA_WAIT_EVERY_N = 5       # add EXTRA_WAIT seconds every N pages\n",
    "EXTRA_WAIT          = 10\n",
    "\n",
    "# Cookies ---------------------------------------------------------------------\n",
    "COOKIE_STR = os.getenv(\"CHAINABUSE_COOKIES\", \"\")  # paste your cf_clearance … here\n",
    "COOKIE_DOMAIN = \".chainabuse.com\"\n",
    "\n",
    "# Playwright viewport & UA -----------------------------------------------------\n",
    "VIEWPORT = {\"width\": 1280, \"height\": 800}\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Logging ---------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log = logging.getLogger(\"chainabuse-scraper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae970486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def parse_cookie_string(cookie_str: str, domain: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Convert a semicolon‑separated cookie string to Playwright's format.\"\"\"\n",
    "    cookies = []\n",
    "    for part in cookie_str.split(\";\"):\n",
    "        part = part.strip()\n",
    "        if \"=\" in part:\n",
    "            name, value = part.split(\"=\", 1)\n",
    "            cookies.append({\"name\": name, \"value\": value, \"domain\": domain, \"path\": \"/\"})\n",
    "    return cookies\n",
    "\n",
    "\n",
    "def extract_cards(html: str) -> List[Tuple[str, str, str, str, str]]:\n",
    "    \"\"\"Return a list of card tuples from the HTML fragment.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    cards = soup.find_all(\"div\", class_=\"create-ScamReportCard\")\n",
    "    results = []\n",
    "    for card in cards:\n",
    "        category = card.find(\"p\", class_=\"create-ScamReportCard__category-label\")\n",
    "        description = card.find(\"p\", class_=\"create-ScamReportCard__preview-description\")\n",
    "        author = card.find(\"a\", class_=\"create-ScamReportCard__author-link\")\n",
    "        btc_addr = card.find(\"div\", class_=\"create-ResponsiveAddress__text\")\n",
    "        domain = card.find(\"p\", class_=\"create-ReportedSection__domain\")\n",
    "        results.append(\n",
    "            (\n",
    "                category.text.strip() if category else \"N/A\",\n",
    "                description.text.strip() if description else \"N/A\",\n",
    "                author.text.strip() if author else \"N/A\",\n",
    "                btc_addr.text.strip() if btc_addr else \"N/A\",\n",
    "                domain.text.strip() if domain else \"N/A\",\n",
    "            )\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbeb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Async scraping routine\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "async def scrape_pages(start_page: int, end_page: int) -> None:\n",
    "    cookies = parse_cookie_string(COOKIE_STR, COOKIE_DOMAIN) if COOKIE_STR else []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(viewport=VIEWPORT, user_agent=USER_AGENT)\n",
    "        if cookies:\n",
    "            await context.add_cookies(cookies)\n",
    "            log.info(\"Injected %s cookies\", len(cookies))\n",
    "\n",
    "        page = await context.new_page()\n",
    "\n",
    "        # Prepare CSV ----------------------------------------------------------\n",
    "        file_exists = CSV_PATH.exists()\n",
    "        csv_file = open(CSV_PATH, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "        writer = csv.writer(csv_file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"page\", \"category\", \"description\", \"author\", \"btc_address\", \"domain\"])\n",
    "\n",
    "        # Determine resume point ----------------------------------------------\n",
    "        if STATE_PATH.exists():\n",
    "            try:\n",
    "                start_page = max(start_page, int(STATE_PATH.read_text().strip()) + 1)\n",
    "                log.info(\"Resuming from page %s\", start_page)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        # Iterate pages --------------------------------------------------------\n",
    "        for pg in tqdm(range(start_page, end_page + 1), desc=\"Scraping pages\"):\n",
    "            url = BASE_URL.format(page=pg)\n",
    "            try:\n",
    "                await page.goto(url)\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "                cards = extract_cards(await page.content())\n",
    "                writer.writerows([(pg, *card) for card in cards])\n",
    "                csv_file.flush()\n",
    "                log.info(\"Page %s – %s cards\", pg, len(cards))\n",
    "\n",
    "                # Persist state\n",
    "                STATE_PATH.write_text(str(pg))\n",
    "\n",
    "                # Politeness delay\n",
    "                delay = WAIT_BETWEEN_PAGES\n",
    "                if pg % EXTRA_WAIT_EVERY_N == 0:\n",
    "                    delay += EXTRA_WAIT\n",
    "                await asyncio.sleep(delay)\n",
    "            except Exception as exc:\n",
    "                log.error(\"Error on page %s: %s\", pg, exc)\n",
    "                await asyncio.sleep(30)\n",
    "\n",
    "        # Cleanup --------------------------------------------------------------\n",
    "        csv_file.close()\n",
    "        await page.close()\n",
    "        await browser.close()\n",
    "        log.info(\"Scraping finished – data written to %s\", CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc609973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Entry point – run this cell\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "async def main():\n",
    "    await scrape_pages(START_PAGE, END_PAGE)\n",
    "\n",
    "# For Jupyter users: run `await main()` directly in the cell below\n",
    "# For script execution: uncomment the following lines:\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
