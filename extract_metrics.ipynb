{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73697e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wallet Pipeline – Fetch + Metrics (Jupyter Edition)\n",
    "===================================================\n",
    "\n",
    "End‑to‑end notebook‑style script that:\n",
    "\n",
    "1. **Downloads** raw transaction data for Elliptic wallets (class 2) from\n",
    "   *Blockchain.info* using a rotating proxy and resilient retries.\n",
    "2. **Validates** all JSON files on disk, discarding corrupt/empty ones.\n",
    "3. **Computes per‑wallet graph & temporal metrics** and writes the final CSV\n",
    "   `elliptic_licit.csv` ready for downstream modelling.\n",
    "\n",
    "> Each `# %%` delimiter denotes a cell when opened in Jupyter Lab/Notebook or\n",
    "> VS Code.  Run cells top‑to‑bottom.  Adapt constants in *Configuration* as\n",
    "> needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fe883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration – tweak as needed\n",
    "# -----------------------------------------------------------------------------\n",
    "DATA_FILE = \"wallets_classes.csv\"      # CSV with `address,class` columns\n",
    "CLASS_FILTER = 2                       # Elliptic: 2 = licit\n",
    "\n",
    "OUTPUT_DIR = Path(\"wallets/elliptic_licit\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROXY_URL = os.getenv(\"ROTATING_PROXY_URL\", \"http://YOUR_ROTATING_PROXY\")\n",
    "\n",
    "MAX_WORKERS = 5            # parallel threads for download\n",
    "BATCH_SIZE = 50            # wallets per batch\n",
    "DELAY_BETWEEN_BATCHES = 2  # s between batches\n",
    "REQUEST_TIMEOUT = 30       # HTTP timeout\n",
    "MAX_RETRIES = 3            # per‑request retries\n",
    "BACKOFF_FACTOR = 1.5       # exponential back‑off multiplier\n",
    "\n",
    "# Logging ---------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "logger = logging.getLogger(\"wallet-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554099d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Fetch helpers – resilient HTTP, skip already‑downloaded wallets\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_wallets(csv_path: str, wallet_class: int) -> List[str]:\n",
    "    \"\"\"Return wallet addresses whose *class* equals *wallet_class*.\"\"\"\n",
    "    with open(csv_path, newline=\"\") as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        return [row[\"address\"] for row in reader if int(row[\"class\"]) == wallet_class]\n",
    "\n",
    "\n",
    "def already_downloaded(out_dir: Path) -> set[str]:\n",
    "    pattern = re.compile(r\"(.+)_transactions\\.json$\")\n",
    "    return {\n",
    "        m.group(1)\n",
    "        for p in out_dir.glob(\"*_transactions.json\")\n",
    "        if (m := pattern.match(p.name))\n",
    "    }\n",
    "\n",
    "\n",
    "def _request_with_retry(session: requests.Session, url: str, retries: int = MAX_RETRIES):\n",
    "    attempt = 0\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            resp = session.get(url, timeout=REQUEST_TIMEOUT)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            if resp.status_code in (429, 500, 502, 503, 504):\n",
    "                delay = BACKOFF_FACTOR ** attempt\n",
    "                logger.warning(\"HTTP %s – retrying in %.1fs\", resp.status_code, delay)\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logger.error(\"Non‑recoverable status %s\", resp.status_code)\n",
    "                return None\n",
    "        except requests.RequestException as exc:\n",
    "            delay = BACKOFF_FACTOR ** attempt\n",
    "            logger.warning(\"%s – retrying in %.1fs\", type(exc).__name__, delay)\n",
    "            time.sleep(delay)\n",
    "        attempt += 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_and_save(wallet: str, out_dir: Path, session: requests.Session) -> bool:\n",
    "    url = f\"https://blockchain.info/rawaddr/{wallet}\"\n",
    "    resp = _request_with_retry(session, url)\n",
    "    if resp and resp.status_code == 200:\n",
    "        out_path = out_dir / f\"{wallet}_transactions.json\"\n",
    "        out_path.write_text(json.dumps(resp.json(), indent=2))\n",
    "        logger.info(\"✅ Saved %s\", out_path.name)\n",
    "        return True\n",
    "    logger.error(\"❌ Failed %s\", wallet)\n",
    "    return False\n",
    "\n",
    "\n",
    "def download_wallets(wallets: Sequence[str], out_dir: Path) -> list[str]:\n",
    "    \"\"\"Download *wallets* and return the list of failed addresses.\"\"\"\n",
    "    failed: list[str] = []\n",
    "    proxy_dict = {\"http\": PROXY_URL, \"https\": PROXY_URL} if PROXY_URL else None\n",
    "    with requests.Session() as session:\n",
    "        if proxy_dict:\n",
    "            session.proxies.update(proxy_dict)\n",
    "\n",
    "        for batch_no, start in enumerate(range(0, len(wallets), BATCH_SIZE), 1):\n",
    "            batch = wallets[start : start + BATCH_SIZE]\n",
    "            logger.info(\"Batch %s/%s – %s wallets\", batch_no, (len(wallets) - 1)//BATCH_SIZE + 1, len(batch))\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "                futures = {pool.submit(fetch_and_save, w, out_dir, session): w for w in batch}\n",
    "                for fut in as_completed(futures):\n",
    "                    if not fut.result():\n",
    "                        failed.append(futures[fut])\n",
    "            if start + BATCH_SIZE < len(wallets):\n",
    "                logger.info(\"Sleeping %ss…\", DELAY_BETWEEN_BATCHES)\n",
    "                time.sleep(DELAY_BETWEEN_BATCHES)\n",
    "    return failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Execute download phase\n",
    "# -----------------------------------------------------------------------------\n",
    "wallet_all = load_wallets(DATA_FILE, CLASS_FILTER)\n",
    "wallet_done = already_downloaded(OUTPUT_DIR)\n",
    "wallet_queue = [w for w in wallet_all if w not in wallet_done]\n",
    "logger.info(\"%s / %s wallets already on disk\", len(wallet_done), len(wallet_all))\n",
    "logger.info(\"Queueing %s wallets for download\", len(wallet_queue))\n",
    "\n",
    "failed_wallets = download_wallets(wallet_queue, OUTPUT_DIR)\n",
    "if failed_wallets:\n",
    "    (OUTPUT_DIR / \"failed_wallets.json\").write_text(json.dumps(sorted(failed_wallets), indent=2))\n",
    "    logger.warning(\"Wrote failed_wallets.json containing %s addresses\", len(failed_wallets))\n",
    "logger.info(\"Download finished – success: %s, failed: %s\", len(wallet_queue) - len(failed_wallets), len(failed_wallets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Validation – build list of JSON files with a non‑empty 'txs' field\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_wallet_transactions(file_path: str):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as fh:\n",
    "            data = json.load(fh)\n",
    "        return data.get(\"txs\", [])\n",
    "    except Exception as exc:\n",
    "        logger.error(\"Invalid JSON %s – %s\", file_path, exc)\n",
    "        return []\n",
    "\n",
    "\n",
    "def list_valid_files(base_dir: Path) -> list[str]:\n",
    "    files = [p for p in base_dir.glob(\"*.json\") if p.is_file()]\n",
    "    valid: list[str] = []\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        futures = {pool.submit(get_wallet_transactions, str(p)): p for p in files}\n",
    "        for fut in tqdm(as_completed(futures), total=len(files), desc=\"Validating JSON\"):\n",
    "            if fut.result():\n",
    "                valid.append(str(futures[fut]))\n",
    "    logger.info(\"%s / %s JSON files contain transactions\", len(valid), len(files))\n",
    "    return valid\n",
    "\n",
    "valid_files = list_valid_files(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Metrics helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def calculate_all_metrics(transactions: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame with one row per (output_address, input_address) pair.\"\"\"\n",
    "    records = []\n",
    "    for tx in transactions:\n",
    "        tx_time = datetime.fromtimestamp(tx[\"time\"])\n",
    "        inputs = [inp[\"prev_out\"].get(\"addr\") for inp in tx[\"inputs\"] if inp[\"prev_out\"].get(\"addr\")]\n",
    "        for out in tx[\"out\"]:\n",
    "            out_addr = out.get(\"addr\")\n",
    "            if out_addr:\n",
    "                for in_addr in inputs:\n",
    "                    records.append({\n",
    "                        \"input_address\": in_addr,\n",
    "                        \"output_address\": out_addr,\n",
    "                        \"amount\": out[\"value\"] / 1e8,\n",
    "                        \"time\": tx_time,\n",
    "                    })\n",
    "    df = pd.DataFrame(records)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df.sort_values(\"time\", inplace=True)\n",
    "    df[\"time_diff\"] = df.groupby(\"output_address\")[\"time\"].diff().dt.total_seconds() / 86400\n",
    "\n",
    "    # Basic aggregates ---------------------------------------------------------\n",
    "    grp_out = df.groupby(\"output_address\")\n",
    "    grp_in  = df.groupby(\"input_address\")\n",
    "\n",
    "    out_deg      = grp_out.size()\n",
    "    in_deg       = grp_in.size()\n",
    "    uniq_in_deg  = grp_out[\"input_address\"].nunique()\n",
    "    uniq_out_deg = grp_in[\"output_address\"].nunique()\n",
    "    avg_in_amt   = grp_out[\"amount\"].mean()\n",
    "    avg_out_amt  = grp_in[\"amount\"].mean()\n",
    "    avg_in_time  = grp_out[\"time_diff\"].mean()\n",
    "    avg_out_time = grp_in[\"time_diff\"].mean()\n",
    "    total_in_amt = grp_out[\"amount\"].sum()\n",
    "    total_out_amt= grp_in[\"amount\"].sum()\n",
    "\n",
    "    # Balance & clustering -----------------------------------------------------\n",
    "    balance = (total_in_amt - total_out_amt.reindex(total_in_amt.index, fill_value=0))\n",
    "    clust_coeff = out_deg / (in_deg.reindex(out_deg.index, fill_value=0) + 1)\n",
    "\n",
    "    # Active duration ----------------------------------------------------------\n",
    "    active_dur = (grp_out[\"time\"].max() - grp_out[\"time\"].min()).dt.total_seconds() / 86400\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        \"in_degree\": out_deg,\n",
    "        \"out_degree\": in_deg.reindex(out_deg.index, fill_value=0),\n",
    "        \"unique_in_degree\": uniq_in_deg,\n",
    "        \"unique_out_degree\": uniq_out_deg.reindex(out_deg.index, fill_value=0),\n",
    "        \"avg_in_transaction\": avg_in_amt,\n",
    "        \"avg_out_transaction\": avg_out_amt.reindex(out_deg.index, fill_value=0),\n",
    "        \"avg_in_time_interval\": avg_in_time,\n",
    "        \"avg_out_time_interval\": avg_out_time.reindex(out_deg.index, fill_value=0),\n",
    "        \"balance\": balance,\n",
    "        \"clustering_coefficient\": clust_coeff,\n",
    "        \"active_duration\": active_dur,\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def aggregate_wallet_metrics(m: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate per‑address metrics to a single‑row wallet‑level DataFrame.\"\"\"\n",
    "    s = {\n",
    "        \"total_in_degree\": m[\"in_degree\"].sum(),\n",
    "        \"total_out_degree\": m[\"out_degree\"].sum(),\n",
    "        \"total_unique_in\": m[\"unique_in_degree\"].sum(),\n",
    "        \"total_unique_out\": m[\"unique_out_degree\"].sum(),\n",
    "        \"avg_in_transaction\": m[\"avg_in_transaction\"].mean(),\n",
    "        \"avg_out_transaction\": m[\"avg_out_transaction\"].mean(),\n",
    "        \"total_received\": m[\"balance\"].clip(lower=0).sum(),\n",
    "        \"total_sent\": (-m[\"balance\"].clip(upper=0)).sum(),\n",
    "        \"net_balance\": m[\"balance\"].sum(),\n",
    "        \"avg_in_time_interval\": m[\"avg_in_time_interval\"].mean(),\n",
    "        \"avg_out_time_interval\": m[\"avg_out_time_interval\"].mean(),\n",
    "        \"avg_active_duration\": m[\"active_duration\"].mean(),\n",
    "    }\n",
    "    return pd.DataFrame([s])\n",
    "\n",
    "\n",
    "def compute_combined_metrics(w: pd.DataFrame) -> pd.DataFrame:\n",
    "    ε = 1e-8\n",
    "    r = w.iloc[0]\n",
    "    combined = {\n",
    "        \"in_out_ratio\": r[\"total_in_degree\"] / (r[\"total_out_degree\"] + ε),\n",
    "        \"unique_in_ratio\": r[\"total_unique_in\"] / (r[\"total_in_degree\"] + ε),\n",
    "        \"unique_out_ratio\": r[\"total_unique_out\"] / (r[\"total_out_degree\"] + ε),\n",
    "        \"volume_ratio\": r[\"total_received\"] / (r[\"total_sent\"] + ε),\n",
    "        \"net_balance_ratio\": r[\"net_balance\"] / (r[\"total_received\"] + ε),\n",
    "        \"activity_index\": (r[\"total_in_degree\"] + r[\"total_out_degree\"]) / (r[\"avg_active_duration\"] + ε),\n",
    "        \"time_interval_ratio\": r[\"avg_out_time_interval\"] / (r[\"avg_in_time_interval\"] + ε),\n",
    "        \"weighted_avg_tx\": (\n",
    "            (r[\"avg_in_transaction\"] * r[\"total_in_degree\"]) +\n",
    "            (r[\"avg_out_transaction\"] * r[\"total_out_degree\"])\n",
    "        ) / (r[\"total_in_degree\"] + r[\"total_out_degree\"] + ε),\n",
    "    }\n",
    "    return pd.DataFrame([combined])\n",
    "\n",
    "\n",
    "def process_wallet(file_path: str):\n",
    "    txs = get_wallet_transactions(file_path)\n",
    "    if not txs:\n",
    "        return None\n",
    "    m = calculate_all_metrics(txs)\n",
    "    if m.empty:\n",
    "        return None\n",
    "    base = aggregate_wallet_metrics(m)\n",
    "    comb = compute_combined_metrics(base)\n",
    "\n",
    "    times = [datetime.fromtimestamp(tx[\"time\"]) for tx in txs]\n",
    "    created, last = min(times), max(times)\n",
    "\n",
    "    df_wallet = pd.concat([base, comb], axis=1)\n",
    "    df_wallet[\"wallet_file\"] = file_path\n",
    "    df_wallet[\"wallet_creation_ts\"] = created.timestamp()\n",
    "    df_wallet[\"wallet_last_tx_ts\"] = last.timestamp()\n",
    "    df_wallet[\"wallet_lifetime_sec\"] = (last - created).total_seconds()\n",
    "    return df_wallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bc1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Run metric computation\n",
    "# -----------------------------------------------------------------------------\n",
    "results: list[pd.DataFrame] = []\n",
    "for fp in tqdm(valid_files, desc=\"Computing metrics\"):\n",
    "    row = process_wallet(fp)\n",
    "    if row is not None:\n",
    "        results.append(row)\n",
    "\n",
    "if not results:\n",
    "    logger.error(\"No metrics could be computed – aborting\")\n",
    "    raise SystemExit\n",
    "\n",
    "metrics_df = pd.concat(results, ignore_index=True)\n",
    "logger.info(\"Final metrics DataFrame shape: %s rows x %s cols\", *metrics_df.shape)\n",
    "\n",
    "# Clean up columns -------------------------------------------------------------\n",
    "metrics_df[\"wallet\"] = metrics_df[\"wallet_file\"].apply(lambda p: Path(p).stem.split(\"_\")[0])\n",
    "metrics_df.drop(columns=[\"wallet_file\"], inplace=True)\n",
    "\n",
    "# Save CSV ---------------------------------------------------------------------\n",
    "CSV_PATH = \"elliptic_licit.csv\"\n",
    "metrics_df.to_csv(CSV_PATH, index=False)\n",
    "logger.info(\"Saved metrics to %s\", CSV_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
