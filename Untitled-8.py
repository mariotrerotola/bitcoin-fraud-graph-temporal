# %%
"""
Wallet Model Training & Explainability Notebook
================================================

End-to-end Jupyter/VS Code notebook that

1. Loads the **processed** dataset generated by *wallet_dataset_preprocessing.py*.
2. Splits the data, builds a preprocessing + Random Forest pipeline, and tunes
   hyper-parameters via `GridSearchCV`.
3. Evaluates the best model with a classification report & confusion matrix.
4. Offers two explainability blocks ‚Äì **SHAP** (commented-out, optional) and
   **CIU** ‚Äì for local feature influence.
5. Saves the trained pipeline (`joblib`) and the report (TXT) alongside the
   dataset for reproducibility.

Key improvements vs. the raw script:
* Uses a single `Pipeline` so the GridSearch tunes the classifier only and keeps
  preprocessing fixed.
* Retrieves feature names automatically via
  `preprocessor.get_feature_names_out()` ‚Äì works even with mixed types.
* Handles older scikit-learn versions: falls back to `sparse=False` if
  `sparse_output` isn‚Äôt available.
* Cleans up unused variables and fixes the invalid attempt to access `.index`
  on NumPy arrays.
* Adds **environment variables** (`DATASET_PATH`, `MODEL_DIR`) to override
  defaults without editing code.
"""

# %%
# ----------------------------------------------------------------------
# Configuration ‚Äì edit paths or set env-vars before starting Jupyter
# ----------------------------------------------------------------------
from __future__ import annotations

import os
from pathlib import Path
import warnings

DATASET_PATH = Path(os.getenv(
    "DATASET_PATH",
    "../../data/processed_data/case_3.csv",
))
MODEL_DIR = Path(os.getenv("MODEL_DIR", DATASET_PATH.parent / "models"))
MODEL_DIR.mkdir(parents=True, exist_ok=True)

REPORT_FILE = MODEL_DIR / "classification_report.txt"
MODEL_FILE  = MODEL_DIR / "rf_pipeline.joblib"

print(f"üìÑ Dataset: {DATASET_PATH}\nüìÇ Model dir: {MODEL_DIR}\n")

# %%
# ----------------------------------------------------------------------
# Imports ‚Äì ML stack & utilities
# ----------------------------------------------------------------------
import logging
import joblib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  # optional for nicer confusion matrix

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Optional explainability libs
# import shap  # uncomment if you plan to use SHAP
import ciu
from IPython.display import display

warnings.filterwarnings("ignore", category=FutureWarning)
logging.basicConfig(level=logging.INFO, format="%(asctime)s ‚Äì %(levelname)s ‚Äì %(message)s")
logger = logging.getLogger(__name__)

# %%
# ----------------------------------------------------------------------
# Load dataset
# ----------------------------------------------------------------------
logger.info("Loading dataset‚Ä¶")
df = pd.read_csv(DATASET_PATH)
logger.info(f"Dataset shape: {df.shape}")

X = df.drop(columns="class")
y = df["class"]

# %%
# ----------------------------------------------------------------------
# Identify numeric & categorical columns
# ----------------------------------------------------------------------
numeric_features      = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
categorical_features  = X.select_dtypes(include=["object", "category"]).columns.tolist()
logger.info(f"Numeric  : {len(numeric_features)} columns")
logger.info(f"Categorical: {len(categorical_features)} columns")

# %%
# ----------------------------------------------------------------------
# Preprocessing pipelines
# ----------------------------------------------------------------------
numeric_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())]
)

# Handle API change for OneHotEncoder (sparse_output introduced in 1.2)
onehot_kwargs = {"handle_unknown": "ignore"}
if "sparse_output" in OneHotEncoder.__init__.__code__.co_varnames:
    onehot_kwargs["sparse_output"] = False
else:
    onehot_kwargs["sparse"] = False

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
        ("onehot", OneHotEncoder(**onehot_kwargs)),
    ]
)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ]
)

# %%
# ----------------------------------------------------------------------
# Model & pipeline
# ----------------------------------------------------------------------
rf = RandomForestClassifier(random_state=42, class_weight="balanced")

pipe = Pipeline(steps=[("preprocessor", preprocessor), ("classifier", rf)])

param_grid = {
    "classifier__n_estimators": [100, 200],
    "classifier__max_depth": [None, 20],
    "classifier__min_samples_split": [2, 5],
    "classifier__min_samples_leaf": [1, 2],
    "classifier__max_features": ["sqrt"],
}

# %%
# ----------------------------------------------------------------------
# Train / validation split & GridSearch
# ----------------------------------------------------------------------
le = LabelEncoder()
y_encoded = le.fit_transform(y)

logger.info("Splitting train/test‚Ä¶")
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42
)

logger.info("Starting GridSearchCV‚Ä¶")
grid_search = GridSearchCV(
    pipe,
    param_grid,
    cv=5,
    scoring="accuracy",
    n_jobs=-1,
    verbose=2,
)

grid_search.fit(X_train, y_train)
logger.info(f"Best params: {grid_search.best_params_}")

best_model = grid_search.best_estimator_

# %%
# ----------------------------------------------------------------------
# Evaluation
# ----------------------------------------------------------------------
y_pred = best_model.predict(X_test)
report = classification_report(y_test, y_pred, target_names=le.classes_)
cm      = confusion_matrix(y_test, y_pred)

print(report)

# Save the textual report
REPORT_FILE.write_text(report)
logger.info(f"Classification report saved ‚Üí {REPORT_FILE}")

# Confusion matrix plot
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix ‚Äì Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# %%
# ----------------------------------------------------------------------
# Persist the trained pipeline
# ----------------------------------------------------------------------
joblib.dump(best_model, MODEL_FILE)
logger.info(f"Model pipeline saved ‚Üí {MODEL_FILE}")

# %% [markdown]
"""
## üîç Local Explainability with CIU
Below cells provide a ready‚Äëmade block to analyse feature influence for a
single test instance using CIU ([*Contextual Importance & Utility*](https://github.com/TrustAI/py‚Äëciu)).

*Change `instance_idx` to explore different samples.*
"""

# %% [markdown]
# 
# ## üîç Local Explainability with CIU
# Below cells provide a ready‚Äëmade block to analyse feature influence for a
# single test instance using CIU ([*Contextual Importance & Utility*](https://github.com/TrustAI/py‚Äëciu)).
# 
# *Change `instance_idx` to explore different samples.*

# %%
# ----------------------------------------------------------------------
# CIU explainability block ‚Äì run as needed
# ----------------------------------------------------------------------
from ciu import CIU

# Fit CIU on the original (non‚Äëencoded) data ‚Äì requires category mapping
category_mapping = {c: X[c].unique().tolist() for c in categorical_features}

ciu_explainer = CIU(
    predictor        = best_model.predict_proba,
    data             = X_train,  # range estimations
    input_names      = best_model.named_steps["preprocessor"].get_feature_names_out(),
    out_names        = le.classes_.tolist(),
    category_mapping = category_mapping,
    neutralCU        = 0.5,
    output_inds      = [0],  # index of positive class
)

# Pick an instance to explain
instance_idx = 0

sample_raw  = X_test.iloc[[instance_idx]]
ciu_result  = ciu_explainer.explain(sample_raw)

print("\n=== CIU ‚Äì local explanation ===")
display(ciu_result.sort_values("Cinfl", ascending=False))

ciu_explainer.plot_influence(
    ciu_result,
    main=f"CIU ‚Äì Feature influence for test sample #{instance_idx}",
)
plt.show()

# %% [markdown]
"""
### Optional: SHAP values
Uncomment the following cell if you prefer SHAP for global explanations (may be
slow on large datasets):
```python
import shap

```
"""


# %%
import shap

# # -------- Option A: generic Explainer on Pipeline.predict --------
# logger.info("Computing SHAP values (generic Explainer)‚Ä¶")
# explainer   = shap.Explainer(best_model.predict, X_train, feature_names=X.columns)
# shap_values = explainer(X_train)

# shap.plots.beeswarm(shap_values, max_display=20)  # global importance

# -------- Option B: TreeExplainer on RF + encoded data --------
logger.info("Computing SHAP values (TreeExplainer)‚Ä¶")
X_train_enc   = best_model.named_steps['preprocessor'].transform(X_train)
feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()
explainer     = shap.TreeExplainer(best_model.named_steps['classifier'])
shap_values   = explainer.shap_values(X_train_enc)

# %%
shap.summary_plot(shap_values[:, :, 0], 
                  features=X_train_enc, 
                  feature_names=feature_names,
                  max_display=20)



