{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090cd1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRandomâ€‘Forest Classification & Featureâ€‘Ablation Study â€“ Jupyter Edition\\n======================================================================\\n\\nTrains a tuned **RandomForestClassifier** on a labelled dataset and\\nsystematically measures performance drops when individual features or\\nfeatureâ€‘pairs are removed (ablation study).\\n\\nWhat this notebook does\\n-----------------------\\n1. **Load & inspect** the dataset specified in `DATA_PATH`.\\n2. **Preâ€‘process** numerical & categorical columns (imputation, scaling,\\n   oneâ€‘hot) via `ColumnTransformer`.\\n3. **Hyperâ€‘parameter search** with `GridSearchCV` (balanced classes).\\n4. **Baseline evaluation** with all features.\\n5. **Perâ€‘feature ablation** â€“ drops one feature at a time and records metrics.\\n6. **Pairwise ablation** â€“ drops every pair of features (âš ï¸\\xa0O(NÂ²) loops).\\n7. **Save results** to CSVs (`ablation_per_feature_results.csv`,\\n   `ablation_pairs_results.csv`).\\n\\n> TIP\\xa0ðŸ’¡\\xa0For large feature sets (>150) the pairwise loop can be extremely slow.\\n> Adjust `MAX_PAIR_FEATURES` below or comment that section if needed.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Randomâ€‘Forest Classification & Featureâ€‘Ablation Study â€“ Jupyter Edition\n",
    "======================================================================\n",
    "\n",
    "Trains a tuned **RandomForestClassifier** on a labelled dataset and\n",
    "systematically measures performance drops when individual features or\n",
    "featureâ€‘pairs are removed (ablation study).\n",
    "\n",
    "What this notebook does\n",
    "-----------------------\n",
    "1. **Load & inspect** the dataset specified in `DATA_PATH`.\n",
    "2. **Preâ€‘process** numerical & categorical columns (imputation, scaling,\n",
    "   oneâ€‘hot) via `ColumnTransformer`.\n",
    "3. **Hyperâ€‘parameter search** with `GridSearchCV` (balanced classes).\n",
    "4. **Baseline evaluation** with all features.\n",
    "5. **Perâ€‘feature ablation** â€“ drops one feature at a time and records metrics.\n",
    "6. **Pairwise ablation** â€“ drops every pair of features (âš ï¸Â O(NÂ²) loops).\n",
    "7. **Save results** to CSVs (`ablation_per_feature_results.csv`,\n",
    "   `ablation_pairs_results.csv`).\n",
    "\n",
    "> TIPÂ ðŸ’¡Â For large feature sets (>150) the pairwise loop can be extremely slow.\n",
    "> Adjust `MAX_PAIR_FEATURES` below or comment that section if needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbd8e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dexire/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc73708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration â€“ edit paths & flags here\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE_PATH = Path(\"../data/processed_data\")\n",
    "USECASE = \"case_1.csv\"\n",
    "DATA_PATH = BASE_PATH / USECASE \n",
    "TARGET_COL = \"class\"          # label column name\n",
    "\n",
    "# Hyperâ€‘parameter grid ---------------------------------------------------------\n",
    "RF_PARAM_GRID = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"criterion\": [\"gini\"],\n",
    "}\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Pairwise ablation guard ------------------------------------------------------\n",
    "MAX_PAIR_FEATURES = 50   # If len(feature_names) exceeds this, skip pair ablation\n",
    "\n",
    "# Output paths ----------------------------------------------------------------\n",
    "OUT_PER_FEATURE = Path(\"ablation_per_feature_results.csv\")\n",
    "OUT_PAIRWISE    = Path(\"ablation_pairs_results.csv\")\n",
    "\n",
    "# Logging ---------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(\"rfâ€‘ablation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a49e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:30:10,968 [INFO] Loaded 15176 rows Ã— 22 columns from ../data/processed_data/case_1.csv\n",
      "2025-06-04 09:30:10,970 [INFO] Target distribution: {'licit': 8833, 'fraud': 6343}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1. Load data\n",
    "# -----------------------------------------------------------------------------\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "log.info(\"Loaded %s rows Ã— %s columns from %s\", *df.shape, DATA_PATH)\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COL}' not found\")\n",
    "\n",
    "X = df.drop(columns=TARGET_COL)\n",
    "y = df[TARGET_COL]\n",
    "log.info(\"Target distribution: %s\", y.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95bf711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:30:14,154 [INFO] Numeric(21): ['total_in_degree', 'total_out_degree', 'total_unique_in', 'total_unique_out', 'avg_in_transaction', 'avg_out_transaction', 'total_received', 'total_sent', 'net_balance', 'avg_in_time_interval', 'avg_out_time_interval', 'avg_active_duration', 'in_out_ratio', 'unique_in_ratio', 'unique_out_ratio', 'volume_ratio', 'net_balance_ratio', 'activity_index', 'time_interval_ratio', 'weighted_avg_tx', 'wallet_lifetime_sec']\n",
      "2025-06-04 09:30:14,155 [INFO] Categorical(0): []\n",
      "2025-06-04 09:30:14,155 [INFO] Fitting preâ€‘processorâ€¦\n",
      "2025-06-04 09:30:14,191 [INFO] Data shape after preprocessing: (15176, 21)\n",
      "2025-06-04 09:30:14,192 [INFO] Total features after encoding: 21\n",
      "2025-06-04 09:30:14,193 [INFO] Label classes: ['fraud', 'licit']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. Preâ€‘processing pipeline\n",
    "# -----------------------------------------------------------------------------\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "log.info(\"Numeric(%s): %s\", len(num_cols), num_cols)\n",
    "log.info(\"Categorical(%s): %s\", len(cat_cols), cat_cols)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols),\n",
    "])\n",
    "\n",
    "log.info(\"Fitting preâ€‘processorâ€¦\")\n",
    "X_prep = preprocessor.fit_transform(X)\n",
    "log.info(\"Data shape after preprocessing: %s\", X_prep.shape)\n",
    "\n",
    "onehot_features = (\n",
    "    preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(cat_cols)\n",
    "    if cat_cols else []\n",
    ")\n",
    "feature_names = np.concatenate([num_cols, onehot_features])\n",
    "log.info(\"Total features after encoding: %s\", len(feature_names))\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder().fit(y)\n",
    "y_enc = le.transform(y)\n",
    "log.info(\"Label classes: %s\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89fba770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:30:17,067 [INFO] Split: 12140 train / 3036 test\n",
      "2025-06-04 09:30:17,067 [INFO] Running GridSearchCV â€¦\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.3s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.6s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.5s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.5s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.0s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.3s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.5s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.5s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   4.4s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.3s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.2s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:30:42,234 [INFO] Best RF params: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. Trainâ€‘test split & hyperâ€‘parameter search\n",
    "# -----------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_prep,\n",
    "    y_enc,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y_enc,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "log.info(\"Split: %s train / %s test\", X_train.shape[0], X_test.shape[0])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "log.info(\"Running GridSearchCV â€¦\")\n",
    "\n",
    "grid = GridSearchCV(rf, RF_PARAM_GRID, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "best_params = grid.best_params_\n",
    "log.info(\"Best RF params: %s\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a966b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:30:49,926 [INFO] Baseline accuracy: 0.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline classification report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       fraud       0.80      0.79      0.80      1269\n",
      "       licit       0.85      0.86      0.85      1767\n",
      "\n",
      "    accuracy                           0.83      3036\n",
      "   macro avg       0.83      0.82      0.83      3036\n",
      "weighted avg       0.83      0.83      0.83      3036\n",
      "\n",
      "Confusion matrix:\n",
      " [[1005  264]\n",
      " [ 252 1515]]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. Baseline model with all features\n",
    "# -----------------------------------------------------------------------------\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "best_rf.fit(X_train, y_train)\n",
    "base_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"=== Baseline classification report ===\")\n",
    "print(classification_report(y_test, base_pred, target_names=list(le.classes_)))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, base_pred))\n",
    "\n",
    "base_acc = accuracy_score(y_test, base_pred)\n",
    "log.info(\"Baseline accuracy: %.4f\", base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c0e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:30:49,931 [INFO] Perâ€‘feature ablation â€¦\n",
      "Ablating single features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [01:36<00:00,  4.60s/it]\n",
      "2025-06-04 09:32:26,534 [INFO] Saved perâ€‘feature ablation results â†’ ablation_per_feature_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Perâ€‘feature ablation\n",
    "# -----------------------------------------------------------------------------\n",
    "log.info(\"Perâ€‘feature ablation â€¦\")\n",
    "per_feat_records = []\n",
    "for idx, feat in enumerate(tqdm(feature_names, desc=\"Ablating single features\")):\n",
    "    keep_idx = [i for i in range(len(feature_names)) if i != idx]\n",
    "    X_tr = X_train[:, keep_idx]\n",
    "    X_te = X_test[:, keep_idx]\n",
    "\n",
    "    clf = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "    clf.fit(X_tr, y_train)\n",
    "    pred = clf.predict(X_te)\n",
    "    report = classification_report(y_test, pred, output_dict=True)\n",
    "\n",
    "    per_feat_records.append({\n",
    "        \"feature_removed\": feat,\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"precision_macro\": report[\"macro avg\"][\"precision\"],\n",
    "        \"recall_macro\": report[\"macro avg\"][\"recall\"],\n",
    "        \"f1_macro\": report[\"macro avg\"][\"f1-score\"],\n",
    "    })\n",
    "\n",
    "per_feat_df = pd.DataFrame(per_feat_records)\n",
    "per_feat_df.to_csv(OUT_PER_FEATURE, index=False)\n",
    "log.info(\"Saved perâ€‘feature ablation results â†’ %s\", OUT_PER_FEATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915d55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:32:26,538 [INFO] Pairwise ablation on 21 features â€¦\n",
      "Ablating pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [16:07<00:00,  4.61s/it]\n",
      "2025-06-04 09:48:33,855 [INFO] Saved pairwise ablation results â†’ ablation_pairs_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 6. Pairwise ablation (optional)\n",
    "# -----------------------------------------------------------------------------\n",
    "if len(feature_names) <= MAX_PAIR_FEATURES:\n",
    "    log.info(\"Pairwise ablation on %s features â€¦\", len(feature_names))\n",
    "    pair_records = []\n",
    "    for i, j in tqdm(list(combinations(range(len(feature_names)), 2)), desc=\"Ablating pairs\"):\n",
    "        keep_idx = [k for k in range(len(feature_names)) if k not in (i, j)]\n",
    "        X_tr = X_train[:, keep_idx]\n",
    "        X_te = X_test[:, keep_idx]\n",
    "        clf = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "        clf.fit(X_tr, y_train)\n",
    "        pred = clf.predict(X_te)\n",
    "        f1 = classification_report(y_test, pred, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "        pair_records.append({\n",
    "            \"features_removed\": f\"{feature_names[i]},{feature_names[j]}\",\n",
    "            \"accuracy\": accuracy_score(y_test, pred),\n",
    "            \"f1_macro\": f1,\n",
    "        })\n",
    "    pair_df = pd.DataFrame(pair_records)\n",
    "    pair_df.to_csv(OUT_PAIRWISE, index=False)\n",
    "    log.info(\"Saved pairwise ablation results â†’ %s\", OUT_PAIRWISE)\n",
    "else:\n",
    "    log.warning(\"Skipped pairwise ablation: %s features > MAX_PAIR_FEATURES=%s\", len(feature_names), MAX_PAIR_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e88cbd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 singleâ€‘feature removals (lowest f1_macro):\n",
      "          feature_removed  f1_macro\n",
      "20    wallet_lifetime_sec  0.809513\n",
      "13        unique_in_ratio  0.817162\n",
      "15           volume_ratio  0.817647\n",
      "10  avg_out_time_interval  0.818570\n",
      "11    avg_active_duration  0.818610\n",
      "\n",
      "Top 5 featureâ€‘pair removals (lowest f1_macro):\n",
      "                            features_removed  f1_macro\n",
      "208  time_interval_ratio,wallet_lifetime_sec  0.803229\n",
      "131           total_sent,wallet_lifetime_sec  0.804266\n",
      "194     unique_out_ratio,wallet_lifetime_sec  0.804583\n",
      "143          net_balance,wallet_lifetime_sec  0.806025\n",
      "19       total_in_degree,wallet_lifetime_sec  0.806025\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7. Quick summary of worst degradations\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nTop 5 singleâ€‘feature removals (lowest f1_macro):\")\n",
    "print(per_feat_df.sort_values(\"f1_macro\").head(5)[[\"feature_removed\", \"f1_macro\"]])\n",
    "\n",
    "if OUT_PAIRWISE.exists():\n",
    "    pair_df = pd.read_csv(OUT_PAIRWISE)\n",
    "    print(\"\\nTop 5 featureâ€‘pair removals (lowest f1_macro):\")\n",
    "    print(pair_df.sort_values(\"f1_macro\").head(5)[[\"features_removed\", \"f1_macro\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dexire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
