{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random‑Forest Classification & Feature‑Ablation Study – Jupyter Edition\n",
    "======================================================================\n",
    "\n",
    "Trains a tuned **RandomForestClassifier** on a labelled dataset and\n",
    "systematically measures performance drops when individual features or\n",
    "feature‑pairs are removed (ablation study).\n",
    "\n",
    "What this notebook does\n",
    "-----------------------\n",
    "1. **Load & inspect** the dataset specified in `DATA_PATH`.\n",
    "2. **Pre‑process** numerical & categorical columns (imputation, scaling,\n",
    "   one‑hot) via `ColumnTransformer`.\n",
    "3. **Hyper‑parameter search** with `GridSearchCV` (balanced classes).\n",
    "4. **Baseline evaluation** with all features.\n",
    "5. **Per‑feature ablation** – drops one feature at a time and records metrics.\n",
    "6. **Pairwise ablation** – drops every pair of features (⚠️ O(N²) loops).\n",
    "7. **Save results** to CSVs (`ablation_per_feature_results.csv`,\n",
    "   `ablation_pairs_results.csv`).\n",
    "\n",
    "> TIP 💡 For large feature sets (>150) the pairwise loop can be extremely slow.\n",
    "> Adjust `MAX_PAIR_FEATURES` below or comment that section if needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc73708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Configuration – edit paths & flags here\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE_PATH = Path(\"../../data/processed_data\")\n",
    "USECASE = \"case_1.csv\"\n",
    "DATA_PATH = BASE_PATH / USECASE \n",
    "TARGET_COL = \"class\"          # label column name\n",
    "\n",
    "# Hyper‑parameter grid ---------------------------------------------------------\n",
    "RF_PARAM_GRID = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"criterion\": [\"gini\"],\n",
    "}\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Pairwise ablation guard ------------------------------------------------------\n",
    "MAX_PAIR_FEATURES = 50   # If len(feature_names) exceeds this, skip pair ablation\n",
    "\n",
    "# Output paths ----------------------------------------------------------------\n",
    "OUT_PER_FEATURE = Path(\"ablation_per_feature_results.csv\")\n",
    "OUT_PAIRWISE    = Path(\"ablation_pairs_results.csv\")\n",
    "\n",
    "# Logging ---------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(\"rf‑ablation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a49e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1. Load data\n",
    "# -----------------------------------------------------------------------------\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "log.info(\"Loaded %s rows × %s columns from %s\", *df.shape, DATA_PATH)\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COL}' not found\")\n",
    "\n",
    "X = df.drop(columns=TARGET_COL)\n",
    "y = df[TARGET_COL]\n",
    "log.info(\"Target distribution: %s\", y.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. Pre‑processing pipeline\n",
    "# -----------------------------------------------------------------------------\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "log.info(\"Numeric(%s): %s\", len(num_cols), num_cols)\n",
    "log.info(\"Categorical(%s): %s\", len(cat_cols), cat_cols)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols),\n",
    "])\n",
    "\n",
    "log.info(\"Fitting pre‑processor…\")\n",
    "X_prep = preprocessor.fit_transform(X)\n",
    "log.info(\"Data shape after preprocessing: %s\", X_prep.shape)\n",
    "\n",
    "onehot_features = (\n",
    "    preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(cat_cols)\n",
    "    if cat_cols else []\n",
    ")\n",
    "feature_names = np.concatenate([num_cols, onehot_features])\n",
    "log.info(\"Total features after encoding: %s\", len(feature_names))\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder().fit(y)\n",
    "y_enc = le.transform(y)\n",
    "log.info(\"Label classes: %s\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fba770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. Train‑test split & hyper‑parameter search\n",
    "# -----------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_prep,\n",
    "    y_enc,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y_enc,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "log.info(\"Split: %s train / %s test\", X_train.shape[0], X_test.shape[0])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "log.info(\"Running GridSearchCV …\")\n",
    "\n",
    "grid = GridSearchCV(rf, RF_PARAM_GRID, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "best_params = grid.best_params_\n",
    "log.info(\"Best RF params: %s\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a966b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4. Baseline model with all features\n",
    "# -----------------------------------------------------------------------------\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "best_rf.fit(X_train, y_train)\n",
    "base_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"=== Baseline classification report ===\")\n",
    "print(classification_report(y_test, base_pred, target_names=list(le.classes_)))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, base_pred))\n",
    "\n",
    "base_acc = accuracy_score(y_test, base_pred)\n",
    "log.info(\"Baseline accuracy: %.4f\", base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Per‑feature ablation\n",
    "# -----------------------------------------------------------------------------\n",
    "log.info(\"Per‑feature ablation …\")\n",
    "per_feat_records = []\n",
    "for idx, feat in enumerate(tqdm(feature_names, desc=\"Ablating single features\")):\n",
    "    keep_idx = [i for i in range(len(feature_names)) if i != idx]\n",
    "    X_tr = X_train[:, keep_idx]\n",
    "    X_te = X_test[:, keep_idx]\n",
    "\n",
    "    clf = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "    clf.fit(X_tr, y_train)\n",
    "    pred = clf.predict(X_te)\n",
    "    report = classification_report(y_test, pred, output_dict=True)\n",
    "\n",
    "    per_feat_records.append({\n",
    "        \"feature_removed\": feat,\n",
    "        \"accuracy\": accuracy_score(y_test, pred),\n",
    "        \"precision_macro\": report[\"macro avg\"][\"precision\"],\n",
    "        \"recall_macro\": report[\"macro avg\"][\"recall\"],\n",
    "        \"f1_macro\": report[\"macro avg\"][\"f1-score\"],\n",
    "    })\n",
    "\n",
    "per_feat_df = pd.DataFrame(per_feat_records)\n",
    "per_feat_df.to_csv(OUT_PER_FEATURE, index=False)\n",
    "log.info(\"Saved per‑feature ablation results → %s\", OUT_PER_FEATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 6. Pairwise ablation (optional)\n",
    "# -----------------------------------------------------------------------------\n",
    "if len(feature_names) <= MAX_PAIR_FEATURES:\n",
    "    log.info(\"Pairwise ablation on %s features …\", len(feature_names))\n",
    "    pair_records = []\n",
    "    for i, j in tqdm(list(combinations(range(len(feature_names)), 2)), desc=\"Ablating pairs\"):\n",
    "        keep_idx = [k for k in range(len(feature_names)) if k not in (i, j)]\n",
    "        X_tr = X_train[:, keep_idx]\n",
    "        X_te = X_test[:, keep_idx]\n",
    "        clf = RandomForestClassifier(**best_params, random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "        clf.fit(X_tr, y_train)\n",
    "        pred = clf.predict(X_te)\n",
    "        f1 = classification_report(y_test, pred, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "        pair_records.append({\n",
    "            \"features_removed\": f\"{feature_names[i]},{feature_names[j]}\",\n",
    "            \"accuracy\": accuracy_score(y_test, pred),\n",
    "            \"f1_macro\": f1,\n",
    "        })\n",
    "    pair_df = pd.DataFrame(pair_records)\n",
    "    pair_df.to_csv(OUT_PAIRWISE, index=False)\n",
    "    log.info(\"Saved pairwise ablation results → %s\", OUT_PAIRWISE)\n",
    "else:\n",
    "    log.warning(\"Skipped pairwise ablation: %s features > MAX_PAIR_FEATURES=%s\", len(feature_names), MAX_PAIR_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7. Quick summary of worst degradations\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nTop 5 single‑feature removals (lowest f1_macro):\")\n",
    "print(per_feat_df.sort_values(\"f1_macro\").head(5)[[\"feature_removed\", \"f1_macro\"]])\n",
    "\n",
    "if OUT_PAIRWISE.exists():\n",
    "    pair_df = pd.read_csv(OUT_PAIRWISE)\n",
    "    print(\"\\nTop 5 feature‑pair removals (lowest f1_macro):\")\n",
    "    print(pair_df.sort_values(\"f1_macro\").head(5)[[\"features_removed\", \"f1_macro\"]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
