{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset Preprocessing Notebook\n",
    "===========================================\n",
    "\n",
    "A cell‚Äëoriented script (compatible with Jupyter¬†Lab/Notebook or VS¬†Code) that\n",
    "combines *licit* and *fraud* wallet feature tables, cleans them, and writes a\n",
    "single labelled dataset.\n",
    "\n",
    "* Reads  **LICIT_WALLETS** and **CHAINABUSE_METRICS** csv files from the data\n",
    "  directory (default `../../../data/data_with_features`).\n",
    "* Adds a `class` column (`\"licit\"` or `\"fraud\"`).\n",
    "* Reports and drops rows containing *any* missing values, with counts before\n",
    "  and after.\n",
    "* Merges the two DataFrames, drops non‚Äëfeature identifier/time columns,\n",
    "  and persists the result to `../../../data/dataset_with_label/chainabuse/`.\n",
    "* Uses **environment variables** `DATA_BASE_PATH` and `OUT_BASE_PATH` to\n",
    "  override defaults.\n",
    "* Designed to be re‚Äëexecuted safely; the output directory is created if\n",
    "  missing and existing files are overwritten.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Configuration ‚Äì set paths & filenames here\n",
    "# ----------------------------------------------------------------------\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base paths (override by exporting env‚Äëvars before starting Jupyter)\n",
    "DATA_BASE_PATH = Path(os.getenv(\"DATA_BASE_PATH\", \"../../../data/data_with_features\"))\n",
    "OUT_BASE_PATH = Path(os.getenv(\"OUT_BASE_PATH\", \"../../../data/dataset_with_label\"))\n",
    "\n",
    "LICIT_FILE = DATA_BASE_PATH / \"licit_wallets_cleaned.csv\"\n",
    "FRAUD_FILE = DATA_BASE_PATH / \"chainabuse_metrics.csv\"\n",
    "\n",
    "OUT_DIR = OUT_BASE_PATH / \"chainabuse\"\n",
    "OUT_FILE = OUT_DIR / \"chainabuse_fraud_unknown_filtered_licit_processed.csv\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÇ Output will be saved to: {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe682fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Imports (data & utility libs)\n",
    "# ----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # show all columns when printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def report_missing(df: pd.DataFrame, name: str) -> None:\n",
    "    \"\"\"Print a summary table of missing value counts for *df* (rows > 0).\"\"\"\n",
    "    nan_counts = df.isna().sum()\n",
    "    nan_df = nan_counts[nan_counts > 0]\n",
    "    if nan_df.empty:\n",
    "        print(f\"‚úÖ {name}: no missing values\\n\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {name}: missing values by column ‚Üí\")\n",
    "        print(nan_df.sort_values(ascending=False).to_frame(\"nan_count\"))\n",
    "        print()\n",
    "\n",
    "\n",
    "def load_and_tag_csv(path: Path, label: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV at *path* and add a constant `class` column=*label*.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"class\"] = label\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee48e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Load licit wallets\n",
    "# ----------------------------------------------------------------------\n",
    "licit_df = load_and_tag_csv(LICIT_FILE, \"licit\")\n",
    "print(f\"üîπ Licit wallets loaded: {len(licit_df):,} rows, {licit_df.shape[1]} columns\")\n",
    "report_missing(licit_df, \"licit_df\")\n",
    "\n",
    "# Drop rows with any NaNs\n",
    "licit_df_clean = licit_df.dropna().reset_index(drop=True)\n",
    "print(f\"üßπ Licit after dropna: {len(licit_df_clean):,} rows (removed {len(licit_df) - len(licit_df_clean):,})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Load fraud wallets (Chainabuse)\n",
    "# ----------------------------------------------------------------------\n",
    "elliptic_fraud_df = load_and_tag_csv(FRAUD_FILE, \"fraud\")\n",
    "print(f\"üî∏ Fraud wallets loaded: {len(elliptic_fraud_df):,} rows, {elliptic_fraud_df.shape[1]} columns\")\n",
    "report_missing(elliptic_fraud_df, \"elliptic_fraud_df\")\n",
    "\n",
    "elliptic_fraud_clean = elliptic_fraud_df.dropna().reset_index(drop=True)\n",
    "print(f\"üßπ Fraud after dropna: {len(elliptic_fraud_clean):,} rows (removed {len(elliptic_fraud_df) - len(elliptic_fraud_clean):,})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Combine datasets\n",
    "# ----------------------------------------------------------------------\n",
    "combined_df = pd.concat([licit_df_clean, elliptic_fraud_clean], ignore_index=True)\n",
    "print(f\"üîó Combined dataset: {len(combined_df):,} rows, {combined_df.shape[1]} columns\")\n",
    "\n",
    "# Columns that are identifiers or timestamps and not useful as features\n",
    "DROP_COLUMNS = [\"wallet\", \"wallet_last_tx_ts\", \"wallet_creation_ts\"]\n",
    "combined_df.drop(columns=[c for c in DROP_COLUMNS if c in combined_df.columns], inplace=True)\n",
    "print(f\"üóëÔ∏è  Dropped columns: {DROP_COLUMNS}\\nCurrent shape: {combined_df.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bbb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Persist to CSV\n",
    "# ----------------------------------------------------------------------\n",
    "combined_df.to_csv(OUT_FILE, index=False)\n",
    "print(f\"üíæ Saved merged dataset ‚Üí {OUT_FILE.relative_to(Path.cwd())}\\n\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "### Next steps\n",
    "* Inspect class balance: `combined_df['class'].value_counts()`\n",
    "* Perform train/test split and feature scaling.\n",
    "* Visualise feature distributions.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
